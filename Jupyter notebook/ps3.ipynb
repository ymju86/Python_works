{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lalonde NSW Data\n",
    "\n",
    "A. Load the Lalonde experimental dataset with the `lalonde_data` method from the module `causalinference.utils`. Using `CausalModel` from the module `causalinference`, provide summary statistics for the outcome variable and the covariates. Which covariate has the largest normalized difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from causalinference.utils import lalonde_data\n",
    "from causalinference import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.555        5.484        6.349        7.867        1.794\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.827        0.379        0.843        0.365        0.044\n",
      "             X1        0.108        0.311        0.059        0.237       -0.175\n",
      "             X2       25.054        7.058       25.816        7.155        0.107\n",
      "             X3        0.154        0.361        0.189        0.393        0.094\n",
      "             X4        0.835        0.372        0.708        0.456       -0.304\n",
      "             X5       10.088        1.614       10.346        2.011        0.141\n",
      "             X6        2.107        5.688        2.096        4.887       -0.002\n",
      "             X7        0.750        0.434        0.708        0.456       -0.094\n",
      "             X8        1.267        3.103        1.532        3.219        0.084\n",
      "             X9        0.685        0.466        0.600        0.491       -0.177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y,D,X = lalonde_data()\n",
    "model = CausalModel(Y,D,X)\n",
    "print(model.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariate `Nodegree` has the largest normalized difference. \n",
    "\n",
    "---\n",
    "\n",
    "B. Estimate the propensity score using the selection algorithm `est_propensity_s`. In selecting the basic covariates set, specify `E74`, `U74`, `E75`, and `U75`. What are the additional linear terms and second-order terms that were selected by the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept     -3.480      4.471     -0.778      0.436    -12.243      5.283\n",
      "            X6      0.034      0.051      0.667      0.505     -0.066      0.133\n",
      "            X7     -0.236      0.386     -0.611      0.541     -0.992      0.521\n",
      "            X8      0.058      0.051      1.144      0.253     -0.041      0.158\n",
      "            X9     -3.477      1.652     -2.104      0.035     -6.716     -0.238\n",
      "            X4      7.329      4.255      1.723      0.085     -1.010     15.668\n",
      "            X1     -0.653      0.385     -1.696      0.090     -1.409      0.102\n",
      "            X5      0.290      0.370      0.783      0.433     -0.435      1.015\n",
      "         X4*X5     -0.668      0.349     -1.915      0.056     -1.352      0.016\n",
      "         X6*X4     -0.130      0.057     -2.286      0.022     -0.241     -0.018\n",
      "         X9*X5      0.304      0.156      1.950      0.051     -0.002      0.609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.est_propensity_s(lin_B=[6,7,8,9])\n",
    "print(model.propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm also selected `Nodegree`, `Hispanic`, `Education`, `Nodegree x Education`, `E74 x Nodegree` and `U75 x Education` for estimating the propensity score. \n",
    "\n",
    "---\n",
    "\n",
    "C. Trim the sample using `trim_s` to get rid of observations with extreme propensity score values. What is the cut-off that is selected? How many observations are dropped as a result? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.131042280162\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.543        5.501        6.237        7.587        1.694\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.828        0.378        0.841        0.367        0.034\n",
      "             X1        0.109        0.313        0.060        0.239       -0.176\n",
      "             X2       25.074        7.091       25.841        7.208        0.107\n",
      "             X3        0.156        0.364        0.187        0.391        0.081\n",
      "             X4        0.832        0.375        0.714        0.453       -0.283\n",
      "             X5       10.105        1.609       10.297        1.964        0.107\n",
      "             X6        1.675        4.435        1.795        3.876        0.029\n",
      "             X7        0.762        0.427        0.714        0.453       -0.108\n",
      "             X8        1.213        3.052        1.457        3.132        0.079\n",
      "             X9        0.691        0.463        0.604        0.490       -0.182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.trim_s()\n",
    "print(model.cutoff)\n",
    "print(model.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cut-off selected is `0.131`. 7 observations are dropped: 4 from the control group and 3 from the treated group.  \n",
    "\n",
    "---\n",
    "\n",
    "D. Stratify the sample using `stratify_s`. How many propensity bins are created? Report the summary statistics for each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.131     0.379       153        67     0.327     0.332     0.788\n",
      "         2     0.380     0.483        69        63     0.435     0.443     1.587\n",
      "         3     0.487     0.852        34        52     0.596     0.619     3.044\n",
      "\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=153)          Treated (N_t=67)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.519        5.821        5.308        6.226        0.788\n",
      "\n",
      "                       Controls (N_c=153)          Treated (N_t=67)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.804        0.398        0.821        0.386        0.043\n",
      "             X1        0.170        0.377        0.104        0.308       -0.190\n",
      "             X2       24.725        7.369       24.955        6.682        0.033\n",
      "             X3        0.144        0.352        0.179        0.386        0.096\n",
      "             X4        0.993        0.081        1.000        0.000        0.114\n",
      "             X5       10.033        1.126       10.194        0.909        0.158\n",
      "             X6        1.571        4.702        0.912        3.003       -0.167\n",
      "             X7        0.817        0.388        0.881        0.327        0.177\n",
      "             X8        0.871        3.065        0.284        1.102       -0.255\n",
      "             X9        0.843        0.365        0.910        0.288        0.205\n",
      "\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "                        Controls (N_c=69)          Treated (N_t=63)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.704        5.060        6.291        8.957        1.587\n",
      "\n",
      "                        Controls (N_c=69)          Treated (N_t=63)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.855        0.355        0.825        0.383       -0.080\n",
      "             X1        0.014        0.120        0.063        0.246        0.253\n",
      "             X2       25.493        6.991       26.079        7.493        0.081\n",
      "             X3        0.159        0.369        0.175        0.383        0.040\n",
      "             X4        0.623        0.488        0.619        0.490       -0.008\n",
      "             X5        9.942        2.169        9.889        2.430       -0.023\n",
      "             X6        0.845        2.453        0.988        2.837        0.054\n",
      "             X7        0.826        0.382        0.794        0.408       -0.082\n",
      "             X8        0.958        2.568        1.427        3.666        0.148\n",
      "             X9        0.623        0.488        0.619        0.490       -0.008\n",
      "\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "                        Controls (N_c=34)          Treated (N_t=52)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.325        4.994        7.369        7.358        3.044\n",
      "\n",
      "                        Controls (N_c=34)          Treated (N_t=52)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.882        0.327        0.885        0.323        0.007\n",
      "             X1        0.029        0.171        0.000        0.000       -0.243\n",
      "             X2       25.794        6.024       26.692        7.519        0.132\n",
      "             X3        0.206        0.410        0.212        0.412        0.014\n",
      "             X4        0.529        0.507        0.462        0.503       -0.134\n",
      "             X5       10.765        1.986       10.923        2.195        0.076\n",
      "             X6        3.830        5.611        3.910        5.048        0.015\n",
      "             X7        0.382        0.493        0.404        0.495        0.043\n",
      "             X8        3.265        3.179        3.003        3.559       -0.078\n",
      "             X9        0.147        0.359        0.192        0.398        0.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.stratify_s()\n",
    "print(model.strata)\n",
    "for stratum in model.strata:\n",
    "    print(stratum.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "E. Estimate the average treatment effect using OLS, blocking, and matching. For matching, set the number of matches to 2 and adjust for bias. How much do the estimates differ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.467      0.638      2.299      0.022      0.216      2.718\n",
      "           ATC      1.385      0.652      2.123      0.034      0.106      2.663\n",
      "           ATT      1.583      0.651      2.432      0.015      0.307      2.858\n",
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.400      0.888      1.576      0.115     -0.341      3.140\n",
      "           ATC      1.316      0.971      1.356      0.175     -0.587      3.219\n",
      "           ATT      1.517      0.935      1.623      0.105     -0.315      3.350\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.542      0.641      2.406      0.016      0.286      2.798\n",
      "           ATC      1.402      0.654      2.145      0.032      0.121      2.683\n",
      "           ATT      1.739      0.663      2.623      0.009      0.440      3.039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.est_via_ols()\n",
    "model.est_via_matching(matches=2, bias_adj=True)\n",
    "model.est_via_blocking()\n",
    "print(model.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates of average treatment effects (ATE) from the three methods are roughly the same . \n",
    "\n",
    "---\n",
    "\n",
    "# Document Classification\n",
    "\n",
    "A. From the module `sklearn.datasets`, load the training data set using the method `fetch_20newsgroups`. This dataset comprises around 18000 newsgroups posts on 20 topics. Print out a couple sample posts and list out all the topic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train')\n",
    "data_test = fetch_20newsgroups(subset='test')\n",
    "Y_train = data_train.target\n",
    "X_train = data_train.data\n",
    "X_test = data_test.data\n",
    "Y_test = data_test.target\n",
    "print(data_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n",
      "From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      "\n",
      "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\n",
      "Subject: Re: Shuttle Launch Question\n",
      "Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\n",
      "Distribution: sci\n",
      "Lines: 23\n",
      "\n",
      "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      ">>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n",
      ">>>\"Clear caution & warning memory.  Verify no unexpected\n",
      ">>>errors. ...\".  I am wondering what an \"expected error\" might\n",
      ">>>be.  Sorry if this is a really dumb question, but\n",
      "> \n",
      "> Parity errors in memory or previously known conditions that were waivered.\n",
      ">    \"Yes that is an error, but we already knew about it\"\n",
      "> I'd be curious as to what the real meaning of the quote is.\n",
      "> \n",
      "> tom\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
      "\n",
      " - Jonathan\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "B. Convert the posts (blobs of texts) into bag-of-word vectors. What is the dimensionality of these vectors? That is, what is the number of words that have appeared in this data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-word vectors have a dimension of `130107` for the training set. However, some of these \"words\" are just meaningless strings. \n",
    "\n",
    "---\n",
    "\n",
    "C. Use your favorite dimensionality reduction technique to compress these vectors into ones of $K=30$ dimensions. \n",
    "\n",
    "D. Use your favorite supervised learning model to train a model that tries to predict the topic of a post from the vectorized representation of the post you obtained in the previous step.\n",
    "\n",
    "E. Use the test data to tune your model. Make sure to include $K$ as a hyperparameter as well. Use `accuracy_score` from `sklearn.metrics` as your evaluation metric. What is the highest accuracy you are able to achieve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=np.zeros(50)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "for k in range(50):\n",
    "    nmf = NMF(n_components=k+1).fit(X_train)\n",
    "    X_train_nmf = nmf.transform(X_train)\n",
    "    X_test_nmf = nmf.transform(X_test)\n",
    "    clf.fit(X_train_nmf, Y_train)\n",
    "    Y_pred = clf.predict(X_test_nmf)\n",
    "    scores[k] = accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHZwUzMysw/8BonpZCKDDk0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222ed8fcd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('NMF + MultiNB')\n",
    "plt.plot(np.array(range(50))+1, scores, 'ro-')\n",
    "plt.xlabel('# of Dimensions')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.530005310674\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(scores)+1,max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835236325013\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Non-negative Matrix Factorization for dimension reduction and Multinomial Naive Bayes for classifier. It turns out that the dimension reduction is not helping the classifier: with up to $K=50$ dimensions, the accuracy score is `0.530`, while using the raw TFIDF vectors the accuracy score is `0.835`. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
